# Scraper de Pel√≠culas de IMDb con Scrapy

Este proyecto implementa un web scraper usando Scrapy para extraer informaci√≥n de las mejores pel√≠culas de IMDb.

## Caracter√≠sticas

- **Framework**: Scrapy (framework profesional de web scraping)
- **Patr√≥n Factory**: Implementaci√≥n del patr√≥n de dise√±o Factory
- **Manejo de errores**: Reintentos autom√°ticos y manejo de WAF
- **Exportaci√≥n dual**: CSV y base de datos PostgreSQL
- **Datos aut√©nticos**: Fallback a datos reales de IMDb cuando el scraping es bloqueado

## Datos Extra√≠dos

Para cada pel√≠cula se extrae:
- ‚úÖ **T√≠tulo**
- ‚úÖ **A√±o de estreno**
- ‚úÖ **Calificaci√≥n** (IMDb rating)
- ‚úÖ **Duraci√≥n en minutos** (desde p√°gina de detalle)
- ‚úÖ **Metascore** (si est√° disponible)
- ‚úÖ **Al menos 3 actores principales**

## üìÅ Estructura del Proyecto
```
imdb_movie_scraper/
‚îú‚îÄ‚îÄ imdb_scraper
‚îú‚îÄ‚îÄ app
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îú‚îÄ‚îÄ db
‚îÇ   ‚îú‚îÄ‚îÄ models
‚îÇ   ‚îú‚îÄ‚îÄ queries
‚îÇ   ‚îú‚îÄ‚îÄ routers
‚îÇ   ‚îú‚îÄ‚îÄ scripts
‚îÇ   ‚îî‚îÄ‚îÄ utils
‚îú‚îÄ‚îÄ data
‚îú‚îÄ‚îÄ vpn-client
‚îú‚îÄ‚îÄ docker-compose.db.yml
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ runtime.txt
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ start.sh
```

## üöÄ Instalaci√≥n y Uso

1. **Clona el repositorio:**

```bash
git clone https://github.com/migherize/imdb-movie-scraper.git
cd app/imdb-movie-scraper
```

2. **Copia el archivo `.env`:**

```bash
cp .env.example .env
```

### ‚ñ∂Ô∏è Opci√≥n 1: Sin Docker

1. **Crea y activa un entorno virtual:**

```bash
python -m venv venv
source venv/bin/activate  # Linux/macOS
venv\Scripts\activate     # Windows
```

2. **Instala dependencias:**

```bash
pip install -r requirements.txt
```

---

### üõ¢Ô∏è **Base de datos PostgreSQL (opcional, sin Docker)**

Si prefieres trabajar con PostgreSQL instalado localmente en lugar de Docker, puedes crear la base de datos y las tablas necesarias ejecutando el script `app/utils/schema.sql`.

#### ‚úÖ Pasos para configurar la base de datos local:

1. **Instala PostgreSQL** (si no lo tienes):

   En Debian/Ubuntu:

   ```bash
   sudo apt update && sudo apt install postgresql postgresql-contrib
   ```

   En macOS con Homebrew:

   ```bash
   brew install postgresql
   brew services start postgresql
   ```

2. **Verifica que el servicio est√© activo:**

   ```bash
   sudo service postgresql status    # Linux
   brew services list                # macOS
   ```

3. **Crea la base de datos `imdb`:**

   ```bash
   createdb imdb
   ```

4. **Ejecuta el script SQL para crear las tablas:**

   ```bash
   psql -U postgres -d imdb -f $(pwd)/app/utils/schema.sql
   ```

   > üîÅ Si usas otro usuario distinto de `postgres`, reempl√°zalo por el tuyo (`-U tu_usuario`).
   >
   > üí° Si te pide contrase√±a, es la de tu usuario PostgreSQL local.

---

#### üìÑ ¬øQu√© incluye `schema.sql`?

Este archivo define la estructura base de la base de datos:

* `movies`: tabla con informaci√≥n de pel√≠culas (t√≠tulo, a√±o, duraci√≥n, rating, metascore).
* `actors`: tabla que almacena los actores, asociados a las pel√≠culas por la clave for√°nea `movie_id`.
* √çndices y restricciones para mejorar la consulta.
* Creaci√≥n de la vista `movie_actor_view` que relaciona pel√≠culas con sus actores principales.

---

schema### üê≥ Opci√≥n 2: Uso con Docker

Puedes levantar la base de datos y el servicio completo usando Docker.

#### 2.1. Levantar la base de datos PostgreSQL:

```bash
docker-compose -f docker-compose.db.yml up --build
```

Esto construir√° y levantar√° el contenedor `imdb-movie-scraper-db-1` con PostgreSQL.

---

#### 2.2. Copiar el archivo `schema.sql` al contenedor:

```bash
docker cp $(pwd)/app/utils/schema.sql imdb-movie-scraper-db-1:/schema.sql
```

> Aseg√∫rate de ajustar la ruta al archivo seg√∫n tu estructura local, si es necesario.

---

#### 2.3. Acceder al contenedor:

```bash
docker exec -it imdb-movie-scraper-db-1 bash
```

---

#### 2.4. Ejecutar el archivo `schema.sql` dentro del contenedor:

```bash
psql -h localhost -U user -d imdb -f schema.sql
```

> Reemplaza `user` y `imdb` por tu usuario y nombre de base de datos si son diferentes.
> Si necesitas contrase√±a, te la pedir√° (`password`, seg√∫n tu `.env`).

---

#### 2. üîå Levantar el servicio IMDB-MOVIE-SCRAPER (opcional)

Si deseas levantar toda la aplicaci√≥n con FastAPI (incluyendo la base de datos y la API), ejecuta:

```bash
docker-compose -f docker-compose.yml up --build
```

Luego, dir√≠gete a la secci√≥n [üöÄ Usar la API con FastAPI (opcional)](#-usar-la-api-con-fastapi-opcional) para m√°s detalles.

---

### ‚úÖ Ejecutar los tests

1. Aseg√∫rate de instalar las dependencias:

```bash
pip install -r requirements.txt
```

2. Ejecuta las pruebas con `pytest`:

```bash
pytest
```

üì∏ Resultado esperado:

![test](docs/test.png)

> **Nota:** El test `test_database_has_data` puede fallar si la base de datos est√° vac√≠a. Esto es normal si a√∫n no has hecho el scraping de datos. El siguiente paso es ejecutar el scraper para poblar la base.

---
Claro, aqu√≠ tienes la secci√≥n completada con una redacci√≥n clara, profesional y consistente con el estilo del resto del README:

---

# üé¨ IMDB Movies Scraper - Aplicaci√≥n de Patrones de Dise√±o en Python

## üìã Descripci√≥n del Proyecto

Este proyecto es un scraper robusto de pel√≠culas de IMDB que implementa m√∫ltiples **patrones de dise√±o estructurales** para crear una arquitectura escalable, mantenible y tolerante a fallos.

# Documentaci√≥n del patr√≥n Factory aplicado en `MovieFactory`

## üì¶ Clase `MovieFactory`

La clase `MovieFactory` es una implementaci√≥n concreta del **Factory Pattern** que se encarga de crear objetos del modelo `Movie` a partir de una estructura de datos tipo diccionario (normalmente una fila o registro extra√≠do de un dataset).

### M√©todo principal

```python
@staticmethod
def create_movie_from_row(row: dict) -> Movie:
  title = str
  year = int
  rating = float
  duration = int
  metascore =float
  actors_raw = str
  return Movie(...)
```

* **Descripci√≥n**: Construye un objeto `Movie` mapeando las claves relevantes de un diccionario `row` hacia los atributos del modelo.
* **Par√°metros**:

  * `row` (dict): Diccionario con datos de una pel√≠cula, con claves como `'title'`, `'date_published'`, `'rating'`, `'duration_minutes'`, `'metascore'`, `'actors'`.
* **Retorna**:

  * Una instancia de la clase `Movie`, con sus campos debidamente inicializados, incluyendo una lista de objetos `Actor` creada a partir del campo `'actors'`.

### Detalles importantes de implementaci√≥n

* Extrae el a√±o a partir de los primeros 4 caracteres de `'date_published'` si est√° presente.
* Convierte los valores de rating, duraci√≥n y metascore a tipos num√©ricos (`float` o `int`) con chequeos para manejar valores nulos.
* Para el campo `'actors'`, que puede ser:

  * Una lista ya construida,
  * Un string con formato de lista (ej: `"['Actor 1', 'Actor 2']"`),
  * O una cadena separada por `;` (ej: `"Actor 1; Actor 2"`),

  se maneja la conversi√≥n segura a una lista de nombres para luego crear los objetos `Actor`.

---

### 1. üè≠ Factory Pattern aplicado en `MovieFactory`

El patr√≥n **Factory** se usa aqu√≠ para centralizar y encapsular la l√≥gica de creaci√≥n compleja de objetos `Movie`. Esto evita que el resto de la aplicaci√≥n tenga que preocuparse por c√≥mo interpretar o validar los datos de entrada.

- **Ubicaci√≥n**: `imdb_movies/imdb_movies/models_patterns/movie_factory.py`

### Ventajas

* **Encapsulamiento**: Toda la l√≥gica de construcci√≥n est√° en un solo lugar.
* **Reutilizaci√≥n**: Se puede reutilizar para crear objetos `Movie` desde distintas fuentes de datos sin duplicar c√≥digo.
* **Mantenibilidad**: Cambios en la forma de construir pel√≠culas o actores s√≥lo afectan esta clase.
* **Robustez**: Maneja distintos formatos y casos de datos inconsistentes de forma controlada.

---

## Ejemplo de uso

```python
row = {
    'title': 'Inception',
    'date_published': '2010-07-16',
    'rating': '8.8',
    'duration_minutes': '148',
    'metascore': '74',
    'actors': "['Leonardo DiCaprio', 'Joseph Gordon-Levitt']"
}

movie = MovieFactory.create_movie_from_row(row)
print(movie.title)  # 'Inception'
print([actor.name for actor in movie.actors])  # ['Leonardo DiCaprio', 'Joseph Gordon-Levitt']
```
---

### 2. üéØ **Strategy Pattern**
- **Ubicaci√≥n**: `imdb_movies/imdb_movies/models_patterns/database_strategies.py`
- **Prop√≥sito**: Permitir el intercambio din√°mico de estrategias para la conexi√≥n a bases de datos, adapt√°ndose a diferentes motores (PostgreSQL, SQLite, MySQL) sin cambiar el c√≥digo cliente. Esto facilita la extensibilidad y el mantenimiento, y permite manejar diferentes configuraciones y fallos de conexi√≥n.
- **Implementaci√≥n**:
  ```python
  class DatabaseStrategy(ABC):
      @abstractmethod
      def get_connection_string(self) -> str: pass
      
      @abstractmethod  
      def get_session(self) -> Session: pass
  ```
- **Estrategias Disponibles**:
  - `PostgreSQLStrategy`
  - `MySQLStrategy`
  - `SQLiteStrategy`
- **Beneficios**:
  - Permite cambiar algoritmos de conexi√≥n en tiempo de ejecuci√≥n
  - Facilita testing con diferentes bases de datos
  - Implementa fallback autom√°tico (PostgreSQL ‚Üí SQLite)

## üõ°Ô∏è Sistema de Manejo de Errores Robusto

### Caracter√≠sticas Implementadas:

#### 1. **Categorizaci√≥n de Errores**
```python
class ErrorType(Enum):
    NETWORK_ERROR = "network_error"
    DATABASE_ERROR = "database_error"
    DATA_VALIDATION_ERROR = "data_validation_error"
    FILE_IO_ERROR = "file_io_error"
    PARSING_ERROR = "parsing_error"
```

#### 2. **Reintentos con Backoff Exponencial**
```python
@retry_with_backoff(
    config=RetryConfig(max_retries=3, base_delay=2.0),
    retry_on=(SQLAlchemyError, DisconnectionError)
)
def critical_operation(self):
    # Operaci√≥n cr√≠tica con reintentos autom√°ticos
```

# Ejecucion

Para poblar la base de datos con informaci√≥n de pel√≠culas y actores:
```bash
cd imdb_scraper

# Solo extracci√≥n
scrapy crawl imdb_movies_spider -a refine=0

# Refinado
scrapy crawl imdb_movies_spider -a refine=1

# Extracci√≥n y refinado
scrapy crawl imdb_movies_spider -a refine=2

# Por defecto es refine=2. Se puede ejecutar:
scrapy crawl imdb_movies_spider
```

Si todo funciona correctamente, deber√≠as ver el siguiente mensaje.

üì∏ Resultado esperado:

![FIN WEBSCRAPY](docs/WEBSCRAPY.png)

---

## üìä Consultas SQL Avanzadas

Este proyecto incluye un conjunto de scripts para ejecutar an√°lisis avanzados sobre la base de datos poblada desde IMDB.

Ejecuta los siguientes comandos desde la carpeta `scripts`:

```bash
python app/scripts/run_query.py -a get_top_movies_by_decade
python app/scripts/run_query.py -a get_standard_deviation_rating
python app/scripts/run_query.py -a get_metascore_and_imdb_rating_normalizado
python app/scripts/run_query.py -a create_view_actor_movie
python app/scripts/run_query.py -a get_view_actor_movie
```

> **Nota:** cada query guarda autom√°ticamente su resultado en la carpeta `data/` con el nombre: `<nombre_de_la_query>.csv`

---

### 1. üé• Top 5 pel√≠culas con mayor duraci√≥n por d√©cada

```bash
python run_query.py -a get_top_movies_by_decade
```

**Descripci√≥n:**
Agrupa las pel√≠culas por d√©cada de estreno y selecciona las 5 pel√≠culas m√°s largas (en duraci√≥n) de cada grupo.

**Objetivo:**
Identificar tendencias temporales en la duraci√≥n de las pel√≠culas y destacar los filmes m√°s extensos por √©poca.

---

### 2. üìà Desviaci√≥n est√°ndar de calificaciones por a√±o

```bash
python run_query.py -a get_standard_deviation_rating
```

**Descripci√≥n:**
Calcula la **desviaci√≥n est√°ndar** de las calificaciones IMDb para cada a√±o.

**Objetivo:**
Medir la dispersi√≥n de opiniones de los usuarios en torno a las pel√≠culas estrenadas cada a√±o. A√±os con mayor desviaci√≥n indican variedad de calidad o polarizaci√≥n.

---

### 3. ‚ö†Ô∏è Diferencias entre calificaciones IMDb y Metascore

```bash
python run_query.py -a get_metascore_and_imdb_rating_normalizado
```

**Descripci√≥n:**
Compara las calificaciones IMDb (escala 1‚Äì10) y Metascore (normalizada a 1‚Äì10) y detecta aquellas pel√≠culas cuya diferencia supera el **20%**.

**Objetivo:**
Detectar discrepancias significativas entre la percepci√≥n del p√∫blico general y la cr√≠tica profesional.

---

### 4. üßë‚Äçü§ù‚Äçüßë Crear vista: Pel√≠culas y actores principales

```bash
python run_query.py -a create_view_actor_movie
```

**Descripci√≥n:**
Crea una **vista SQL** que relaciona cada pel√≠cula con sus actores principales.

**Objetivo:**
Facilitar consultas r√°pidas de pel√≠culas en las que participa un actor espec√≠fico, sin necesidad de m√∫ltiples `JOIN`.

> üí° Este paso **debe ejecutarse solo una vez** para crear la vista.

---

### 5. üîé Consultar vista: Filtrar por actor

```bash
python run_query.py -a get_view_actor_movie
```

**Descripci√≥n:**
Consulta la vista creada anteriormente, permitiendo filtrar pel√≠culas por nombre de actor.

**Objetivo:**
Explorar f√°cilmente la filmograf√≠a de un actor y su rol en distintas pel√≠culas.

---

¬°Claro! Aqu√≠ tienes un ejemplo claro y paso a paso para tu README, explicando la parte de Proxies & Control de Red con la estructura de carpetas, qu√© hace cada archivo y c√≥mo probar la rotaci√≥n de IP y logs.

---

# Proxies & Control de Red (10%)

Para garantizar la continuidad y efectividad del scraping evitando bloqueos por parte de los sitios objetivo, se implement√≥ una estrategia robusta de gesti√≥n de red y control de IPs que incluye la integraci√≥n de VPN mediante Docker y proxies rotativos.

---

## Estructura de la carpeta `vpn-client`

```
vpn-client
‚îú‚îÄ‚îÄ auth.txt
‚îú‚îÄ‚îÄ check_country.sh
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ example.ovpn
```

* **auth.txt**: Archivo que contiene tus credenciales para autenticar la conexi√≥n VPN (usuario y contrase√±a).
* **check\_country.sh**: Script que verifica que la IP p√∫blica actual del contenedor corresponde al pa√≠s esperado usando una API p√∫blica.
* **Dockerfile**: Define la imagen Docker que instala OpenVPN y configura el contenedor para conectarse a la VPN, adem√°s de ejecutar el healthcheck.
* **example.ovpn**: Archivos de configuraci√≥n OpenVPN para conectarse a diferentes servidores y pa√≠ses seg√∫n el proveedor VPN.

---

## Descripci√≥n de los archivos clave

### `auth.txt`

Contiene las credenciales necesarias para la autenticaci√≥n en el servidor VPN:

```
usuario_vpn
contrase√±a_vpn
```

---

### `check_country.sh`

Script que se ejecuta peri√≥dicamente para validar que la IP p√∫blica dentro del contenedor corresponde al pa√≠s configurado.

```bash
#!/bin/bash

EXPECTED="Japan"
IP=$(curl -s https://api.ipify.org)
COUNTRY=$(curl -s https://ipapi.co/$IP/country_name/)

if [[ "$COUNTRY" == "$EXPECTED" ]]; then
    echo "‚úÖ Connected to $COUNTRY ($IP)"
    exit 0
else
    echo "‚ùå Connected to wrong country: $COUNTRY ($IP)"
    exit 1
fi
```

---

### `Dockerfile`

El Dockerfile crea una imagen basada en Ubuntu 20.04 con OpenVPN y herramientas necesarias instaladas, configura DNS para evitar problemas de resoluci√≥n, copia los archivos de configuraci√≥n y define el comando para iniciar OpenVPN con autenticaci√≥n.

```dockerfile
FROM ubuntu:20.04

RUN apt-get update && \
    apt-get install -y openvpn curl iproute2 iputils-ping && \
    echo "nameserver 1.1.1.1" > /etc/resolv.conf && \
    rm -rf /var/lib/apt/lists/*

ENV OVPN_FILE=example.ovpn

COPY ${OVPN_FILE} /vpn-client/${OVPN_FILE}
COPY auth.txt /vpn-client/auth.txt
COPY check_country.sh /usr/local/bin/check_country.sh

RUN chmod +x /usr/local/bin/check_country.sh

CMD ["sh", "-c", "openvpn --config /vpn-client/${OVPN_FILE} --auth-user-pass /vpn-client/auth.txt"]

HEALTHCHECK --interval=30s --timeout=10s --retries=3 CMD /usr/local/bin/check_country.sh
```

---

## C√≥mo usarlo

1. **Configura tus credenciales VPN en `auth.txt`.**

2. **Elige o a√±ade el archivo `.ovpn` correspondiente al pa√≠s o servidor deseado.**

3. **Construye la imagen Docker:**

```bash
docker build -t vpn-client .
```

4. **Ejecuta el contenedor con permisos para usar la VPN:**

```bash
docker run --cap-add=NET_ADMIN --device /dev/net/tun --name vpn-client vpn-client
```

5. **Verifica desde dentro del contenedor la IP p√∫blica y pa√≠s:**

```bash
docker exec -it vpn-client curl https://api.ipify.org
```

---

## üß† Comparaci√≥n T√©cnica: Scrapy vs Selenium vs Playwright

### ¬øC√≥mo implementar√≠as este scraper usando **Playwright** o **Selenium**?

### 1. **Configuraci√≥n avanzada del navegador**

* En Scrapy usas headers HTTP muy completos que simulan un navegador real (User-Agent, sec-ch-ua, sec-fetch, etc.) para evitar bloqueos o detecci√≥n.
* En Playwright o Selenium, en vez de solo enviar headers, **configuras un navegador real (Chromium, Firefox, WebKit)** que autom√°ticamente maneja la mayor√≠a de esos headers y cookies de forma nativa, con mayor fidelidad.
* Puedes **inyectar cookies expl√≠citamente** antes de navegar para mantener sesiones o estados (igual que en Scrapy con `COOKIES`).
* Adem√°s, puedes usar opciones como:
  * `headless=True/False` para correr visible o no.
  * Plugins o scripts para hacer stealth (ocultar `navigator.webdriver`, evitar detecci√≥n de bots).

Esto da una ventaja sobre Scrapy porque no s√≥lo "simulas" headers, sino que usas un navegador completo.

---

### 2. **Selectores din√°micos y espera expl√≠cita**

* Scrapy es r√°pido para p√°ginas est√°ticas, pero con contenido din√°mico (JS) puede fallar.
* Con Playwright/Selenium puedes usar:

  * `wait_for_selector(xpath)` o `WebDriverWait` para esperar hasta que los elementos est√©n cargados.
  * Esto es cr√≠tico para p√°ginas como IMDb que pueden tener elementos generados o scripts que modifican el DOM.

Esto te permite manejar con precisi√≥n cu√°ndo extraer los datos JSON que est√°n en el XPath `//script[@type='application/ld+json']`.

---

### 3. **Soporte completo para JavaScript y CAPTCHA**

* IMDb puede usar JavaScript para cargar informaci√≥n o protecci√≥n anti-bots.
* Con Playwright/Selenium puedes:
  * Ejecutar scripts JS directamente.
  * Interactuar con la p√°gina para hacer login, aceptar cookies, resolver captchas (usando servicios externos).
* Esto no es posible solo con Scrapy y los headers, ya que Scrapy no ejecuta JS.

---

### 4. **Control de concurrencia**

* Scrapy es muy eficiente para scraping concurrente nativo.
* Playwright soporta concurrencia asincr√≥nica (`async`), lo que permite lanzar varias instancias navegando simult√°neamente.
* Selenium puede usar `ThreadPoolExecutor` o procesos para concurrencia.
* Para scraping masivo puedes combinar Playwright/Selenium con colas como Celery.

Esto es √∫til si quieres escalar tu scraping m√°s all√° de un solo hilo o proceso.

---

### üü¢ ¬øPor qu√© usamos Scrapy?

* Scrapy es m√°s **ligero y r√°pido** para sitios est√°ticos como IMDB.
* Tiene soporte nativo para middlewares, manejo de errores, y pipelines de datos.
* Mejor integraci√≥n con proyectos de an√°lisis de datos y control de volumen con `AutoThrottle`.

---

## üöÄ Usar la API con FastAPI (opcional)

Una vez poblada la base de datos, puedes consultar los datos f√°cilmente desde la API REST:

![API](docs/API.png)

### ‚ñ∂Ô∏è Opci√≥n 1: Sin Docker

1. **Ejecuta la aplicaci√≥n:**
```
uvicorn app.main:app --reload
```

### üê≥ Opci√≥n 2: Uso con Docker

1. **Accede a la documentaci√≥n interactiva de FastAPI:**

```
http://localhost:8080/docs
```

Desde all√≠ puedes ejecutar cada query directamente desde el navegador y obtener resultados en JSON.

üì∏ Ejemplo:

![example](docs/example.png)
