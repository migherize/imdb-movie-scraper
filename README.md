# Scraper de Pel√≠culas de IMDb con Scrapy

Este proyecto implementa un web scraper usando Scrapy para extraer informaci√≥n de las mejores pel√≠culas de IMDb.

## Caracter√≠sticas

- **Framework**: Scrapy (framework profesional de web scraping)
- **Patr√≥n Factory**: Implementaci√≥n del patr√≥n de dise√±o Factory
- **Manejo de errores**: Reintentos autom√°ticos y manejo de WAF
- **Exportaci√≥n dual**: CSV y base de datos PostgreSQL
- **Datos aut√©nticos**: Fallback a datos reales de IMDb cuando el scraping es bloqueado

## Datos Extra√≠dos

Para cada pel√≠cula se extrae:
- ‚úÖ **T√≠tulo**
- ‚úÖ **A√±o de estreno**
- ‚úÖ **Calificaci√≥n** (IMDb rating)
- ‚úÖ **Duraci√≥n en minutos** (desde p√°gina de detalle)
- ‚úÖ **Metascore** (si est√° disponible)
- ‚úÖ **Al menos 3 actores principales**

## üìÅ Estructura del Proyecto
```
imdb_movie_scraper/
‚îú‚îÄ‚îÄ imdb_scraper
‚îú‚îÄ‚îÄ app
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îú‚îÄ‚îÄ db
‚îÇ   ‚îú‚îÄ‚îÄ models
‚îÇ   ‚îú‚îÄ‚îÄ queries
‚îÇ   ‚îú‚îÄ‚îÄ routers
‚îÇ   ‚îú‚îÄ‚îÄ scripts
‚îÇ   ‚îî‚îÄ‚îÄ utils
‚îî‚îÄ‚îÄ start.sh
‚îú‚îÄ‚îÄ data
‚îú‚îÄ‚îÄ docker-compose.db.yml
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ runtime.txt
‚îú‚îÄ‚îÄ README.md
```

## üöÄ Instalaci√≥n y Uso

1. **Clona el repositorio:**

```bash
git clone https://github.com/migherize/imdb-movie-scraper.git
cd imdb-movie-scraper
```

2. **Copia el archivo `.env`:**

```bash
cp .env.example .env
```

### ‚ñ∂Ô∏è Opci√≥n 1: Sin Docker

1. **Crea y activa un entorno virtual:**

```bash
python -m venv venv
source venv/bin/activate  # Linux/macOS
venv\Scripts\activate     # Windows
```

2. **Instala dependencias:**

```bash
pip install -r requirements.txt
```

---

### üõ¢Ô∏è **Base de datos PostgreSQL (modo opcional)**

Si deseas usar PostgreSQL localmente (sin Docker), puedes crear la base de datos y sus tablas ejecutando el script `app/utils/schema.sql` incluido en el proyecto.

#### ‚úÖ Pasos:

1. **Aseg√∫rate de tener PostgreSQL instalado** y que el servicio est√© en ejecuci√≥n.

2. **Crea la base de datos manualmente** (si a√∫n no existe):

   ```bash
   createdb imdb
   ```

3. **Ejecuta el script SQL para crear las tablas:**

   ```bash
   psql -h localhost -U tu_usuario -d imdb -f app/utils/schema.sql
   ```

   > Reemplaza `tu_usuario` por tu nombre de usuario en PostgreSQL.

#### üìÑ ¬øQu√© incluye `schema.sql`?

Este archivo define la estructura base de la base de datos:

* `movies`: tabla con informaci√≥n de pel√≠culas (t√≠tulo, a√±o, duraci√≥n, rating, metascore).
* `actors`: tabla que almacena los actores, asociados a las pel√≠culas por la clave for√°nea `movie_id`.
* √çndices y restricciones para mejorar la consulta.
* Creaci√≥n de la vista `movie_actor_view` que relaciona pel√≠culas con sus actores principales.

---

### üê≥ Opci√≥n 2: Uso con Docker

Puedes levantar la base de datos y el servicio completo usando Docker.

#### 1. Levantar la base de datos PostgreSQL:

```bash
docker-compose -f docker-compose.db.yml up --build
```

Esto construir√° y levantar√° el contenedor `imdb-movie-scraper-db-1` con PostgreSQL.

---

#### 2. Copiar el archivo `schema.sql` al contenedor:

```bash
docker cp PYTHONPATH/app/utils/schema.sql imdb-movie-scraper-db-1:/schema.sql
```

> Aseg√∫rate de ajustar la ruta al archivo seg√∫n tu estructura local, si es necesario.

---

#### 3. Acceder al contenedor:

```bash
docker exec -it imdb-movie-scraper-db-1 bash
```

---

#### 4. Ejecutar el archivo `schema.sql` dentro del contenedor:

```bash
psql -h localhost -U user -d imdb -f schema.sql
```

> Reemplaza `user` y `imdb` por tu usuario y nombre de base de datos si son diferentes.
> Si necesitas contrase√±a, te la pedir√° (`password`, seg√∫n tu `.env`).

---

#### 2. üîå Levantar el servicio IMDB-MOVIE-SCRAPER (opcional)

Si deseas levantar toda la aplicaci√≥n con FastAPI (incluyendo la base de datos y la API), ejecuta:

```bash
docker-compose -f docker-compose.yml up --build
```

Luego, dir√≠gete a la secci√≥n [üöÄ Usar la API con FastAPI (opcional)](#-usar-la-api-con-fastapi-opcional) para m√°s detalles.

---

### ‚úÖ Ejecutar los tests

1. Aseg√∫rate de instalar las dependencias:

```bash
pip install -r requirements.txt
```

2. Ejecuta las pruebas con `pytest`:

```bash
pytest
```

üì∏ Resultado esperado:

![test](docs/test.png)

> **Nota:** El test `test_database_has_data` puede fallar si la base de datos est√° vac√≠a. Esto es normal si a√∫n no has hecho el scraping de datos. El siguiente paso es ejecutar el scraper para poblar la base.

---
Claro, aqu√≠ tienes la secci√≥n completada con una redacci√≥n clara, profesional y consistente con el estilo del resto del README:

---

## üé¨ Web Scraper: IMDB Movies

Para poblar la base de datos con informaci√≥n de pel√≠culas y actores:

```bash
cd imdb_scraper
scrapy crawl imdb_movies_spider
```

Si todo funciona correctamente, deber√≠as ver el siguiente mensaje.

üì∏ Resultado esperado:

![FIN WEBSCRAPY](docs/WEBSCRAPY.png)

---

## üìä Consultas SQL Avanzadas

Este proyecto incluye un conjunto de scripts para ejecutar an√°lisis avanzados sobre la base de datos poblada desde IMDB.

Ejecuta los siguientes comandos desde la carpeta `scripts`:

```bash
cd scripts
python app/scripts/run_query.py -a get_top_movies_by_decade
python app/scripts/run_query.py -a get_standard_deviation_rating
python app/scripts/run_query.py -a get_metascore_and_imdb_rating_normalizado
python app/scripts/run_query.py -a create_view_actor_movie
python app/scripts/run_query.py -a get_view_actor_movie
```

> **Nota:** cada query guarda autom√°ticamente su resultado en la carpeta `data/` con el nombre: `<nombre_de_la_query>.csv`

---

### 1. üé• Top 5 pel√≠culas con mayor duraci√≥n por d√©cada

```bash
python run_query.py -a get_top_movies_by_decade
```

**Descripci√≥n:**
Agrupa las pel√≠culas por d√©cada de estreno y selecciona las 5 pel√≠culas m√°s largas (en duraci√≥n) de cada grupo.

**Objetivo:**
Identificar tendencias temporales en la duraci√≥n de las pel√≠culas y destacar los filmes m√°s extensos por √©poca.

---

### 2. üìà Desviaci√≥n est√°ndar de calificaciones por a√±o

```bash
python run_query.py -a get_standard_deviation_rating
```

**Descripci√≥n:**
Calcula la **desviaci√≥n est√°ndar** de las calificaciones IMDb para cada a√±o.

**Objetivo:**
Medir la dispersi√≥n de opiniones de los usuarios en torno a las pel√≠culas estrenadas cada a√±o. A√±os con mayor desviaci√≥n indican variedad de calidad o polarizaci√≥n.

---

### 3. ‚ö†Ô∏è Diferencias entre calificaciones IMDb y Metascore

```bash
python run_query.py -a get_metascore_and_imdb_rating_normalizado
```

**Descripci√≥n:**
Compara las calificaciones IMDb (escala 1‚Äì10) y Metascore (normalizada a 1‚Äì10) y detecta aquellas pel√≠culas cuya diferencia supera el **20%**.

**Objetivo:**
Detectar discrepancias significativas entre la percepci√≥n del p√∫blico general y la cr√≠tica profesional.

---

### 4. üßë‚Äçü§ù‚Äçüßë Crear vista: Pel√≠culas y actores principales

```bash
python run_query.py -a create_view_actor_movie
```

**Descripci√≥n:**
Crea una **vista SQL** que relaciona cada pel√≠cula con sus actores principales.

**Objetivo:**
Facilitar consultas r√°pidas de pel√≠culas en las que participa un actor espec√≠fico, sin necesidad de m√∫ltiples `JOIN`.

> üí° Este paso **debe ejecutarse solo una vez** para crear la vista.

---

### 5. üîé Consultar vista: Filtrar por actor

```bash
python run_query.py -a get_view_actor_movie
```

**Descripci√≥n:**
Consulta la vista creada anteriormente, permitiendo filtrar pel√≠culas por nombre de actor.

**Objetivo:**
Explorar f√°cilmente la filmograf√≠a de un actor y su rol en distintas pel√≠culas.

---

¬°Claro! Aqu√≠ tienes un ejemplo claro y paso a paso para tu README, explicando la parte de Proxies & Control de Red con la estructura de carpetas, qu√© hace cada archivo y c√≥mo probar la rotaci√≥n de IP y logs.

---

# Proxies & Control de Red (10%)

Para garantizar la continuidad y efectividad del scraping evitando bloqueos por parte de los sitios objetivo, se implement√≥ una estrategia robusta de gesti√≥n de red y control de IPs que incluye la integraci√≥n de VPN mediante Docker y proxies rotativos.

---

## Estructura de la carpeta `vpn-client`

```
vpn-client
‚îú‚îÄ‚îÄ auth.txt
‚îú‚îÄ‚îÄ check_country.sh
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ example.ovpn
```

* **auth.txt**: Archivo que contiene tus credenciales para autenticar la conexi√≥n VPN (usuario y contrase√±a).
* **check\_country.sh**: Script que verifica que la IP p√∫blica actual del contenedor corresponde al pa√≠s esperado usando una API p√∫blica.
* **Dockerfile**: Define la imagen Docker que instala OpenVPN y configura el contenedor para conectarse a la VPN, adem√°s de ejecutar el healthcheck.
* **example.ovpn**: Archivos de configuraci√≥n OpenVPN para conectarse a diferentes servidores y pa√≠ses seg√∫n el proveedor VPN.

---

## Descripci√≥n de los archivos clave

### `auth.txt`

Contiene las credenciales necesarias para la autenticaci√≥n en el servidor VPN:

```
usuario_vpn
contrase√±a_vpn
```

---

### `check_country.sh`

Script que se ejecuta peri√≥dicamente para validar que la IP p√∫blica dentro del contenedor corresponde al pa√≠s configurado.

```bash
#!/bin/bash

EXPECTED="Japan"
IP=$(curl -s https://api.ipify.org)
COUNTRY=$(curl -s https://ipapi.co/$IP/country_name/)

if [[ "$COUNTRY" == "$EXPECTED" ]]; then
    echo "‚úÖ Connected to $COUNTRY ($IP)"
    exit 0
else
    echo "‚ùå Connected to wrong country: $COUNTRY ($IP)"
    exit 1
fi
```

---

### `Dockerfile`

El Dockerfile crea una imagen basada en Ubuntu 20.04 con OpenVPN y herramientas necesarias instaladas, configura DNS para evitar problemas de resoluci√≥n, copia los archivos de configuraci√≥n y define el comando para iniciar OpenVPN con autenticaci√≥n.

```dockerfile
FROM ubuntu:20.04

RUN apt-get update && \
    apt-get install -y openvpn curl iproute2 iputils-ping && \
    echo "nameserver 1.1.1.1" > /etc/resolv.conf && \
    rm -rf /var/lib/apt/lists/*

ENV OVPN_FILE=proton.ovpn

COPY ${OVPN_FILE} /vpn-client/${OVPN_FILE}
COPY auth.txt /vpn-client/auth.txt
COPY check_country.sh /usr/local/bin/check_country.sh

RUN chmod +x /usr/local/bin/check_country.sh

CMD ["sh", "-c", "openvpn --config /vpn-client/${OVPN_FILE} --auth-user-pass /vpn-client/auth.txt"]

HEALTHCHECK --interval=30s --timeout=10s --retries=3 CMD /usr/local/bin/check_country.sh
```

---

## C√≥mo usarlo

1. **Configura tus credenciales VPN en `auth.txt`.**

2. **Elige o a√±ade el archivo `.ovpn` correspondiente al pa√≠s o servidor deseado.**

3. **Construye la imagen Docker:**

```bash
docker build -t vpn-client .
```

4. **Ejecuta el contenedor con permisos para usar la VPN:**

```bash
docker run --cap-add=NET_ADMIN --device /dev/net/tun --name vpn-client vpn-client
```

5. **Verifica desde dentro del contenedor la IP p√∫blica y pa√≠s:**

```bash
docker exec -it vpn-client curl https://api.ipify.org
```

---

## üß† Comparaci√≥n T√©cnica: Scrapy vs Selenium vs Playwright

### ¬øC√≥mo implementar√≠as este scraper usando **Playwright** o **Selenium**?

### 1. **Configuraci√≥n avanzada del navegador**

* En Scrapy usas headers HTTP muy completos que simulan un navegador real (User-Agent, sec-ch-ua, sec-fetch, etc.) para evitar bloqueos o detecci√≥n.
* En Playwright o Selenium, en vez de solo enviar headers, **configuras un navegador real (Chromium, Firefox, WebKit)** que autom√°ticamente maneja la mayor√≠a de esos headers y cookies de forma nativa, con mayor fidelidad.
* Puedes **inyectar cookies expl√≠citamente** antes de navegar para mantener sesiones o estados (igual que en Scrapy con `COOKIES`).
* Adem√°s, puedes usar opciones como:
  * `headless=True/False` para correr visible o no.
  * Plugins o scripts para hacer stealth (ocultar `navigator.webdriver`, evitar detecci√≥n de bots).

Esto da una ventaja sobre Scrapy porque no s√≥lo "simulas" headers, sino que usas un navegador completo.

---

### 2. **Selectores din√°micos y espera expl√≠cita**

* Scrapy es r√°pido para p√°ginas est√°ticas, pero con contenido din√°mico (JS) puede fallar.
* Con Playwright/Selenium puedes usar:

  * `wait_for_selector(xpath)` o `WebDriverWait` para esperar hasta que los elementos est√©n cargados.
  * Esto es cr√≠tico para p√°ginas como IMDb que pueden tener elementos generados o scripts que modifican el DOM.

Esto te permite manejar con precisi√≥n cu√°ndo extraer los datos JSON que est√°n en el XPath `//script[@type='application/ld+json']`.

---

### 3. **Soporte completo para JavaScript y CAPTCHA**

* IMDb puede usar JavaScript para cargar informaci√≥n o protecci√≥n anti-bots.
* Con Playwright/Selenium puedes:
  * Ejecutar scripts JS directamente.
  * Interactuar con la p√°gina para hacer login, aceptar cookies, resolver captchas (usando servicios externos).
* Esto no es posible solo con Scrapy y los headers, ya que Scrapy no ejecuta JS.

---

### 4. **Control de concurrencia**

* Scrapy es muy eficiente para scraping concurrente nativo.
* Playwright soporta concurrencia asincr√≥nica (`async`), lo que permite lanzar varias instancias navegando simult√°neamente.
* Selenium puede usar `ThreadPoolExecutor` o procesos para concurrencia.
* Para scraping masivo puedes combinar Playwright/Selenium con colas como Celery.

Esto es √∫til si quieres escalar tu scraping m√°s all√° de un solo hilo o proceso.

---

### üü¢ ¬øPor qu√© usamos Scrapy?

* Scrapy es m√°s **ligero y r√°pido** para sitios est√°ticos como IMDB.
* Tiene soporte nativo para middlewares, manejo de errores, y pipelines de datos.
* Mejor integraci√≥n con proyectos de an√°lisis de datos y control de volumen con `AutoThrottle`.

---

## üöÄ Usar la API con FastAPI (opcional)

Una vez poblada la base de datos, puedes consultar los datos f√°cilmente desde la API REST:

![API](docs/API.png)

### ‚ñ∂Ô∏è Opci√≥n 1: Sin Docker

1. **Ejecuta la aplicaci√≥n:**
```
uvicorn app.main:app --reload
```

### üê≥ Opci√≥n 2: Uso con Docker

1. **Accede a la documentaci√≥n interactiva de FastAPI:**

```
http://localhost:8080/docs
```

Desde all√≠ puedes ejecutar cada query directamente desde el navegador y obtener resultados en JSON.

üì∏ Ejemplo:

![example](docs/example.png)
