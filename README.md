# Scraper de PelÃ­culas de IMDb con Scrapy

Este proyecto implementa un web scraper usando Scrapy para extraer informaciÃ³n de las mejores pelÃ­culas de IMDb.

## CaracterÃ­sticas

- **Framework**: Scrapy (framework profesional de web scraping)
- **PatrÃ³n Factory**: ImplementaciÃ³n del patrÃ³n de diseÃ±o Factory
- **Manejo de errores**: Reintentos automÃ¡ticos y manejo de WAF
- **ExportaciÃ³n dual**: CSV y base de datos PostgreSQL
- **Datos autÃ©nticos**: Fallback a datos reales de IMDb cuando el scraping es bloqueado

## Datos ExtraÃ­dos

Para cada pelÃ­cula se extrae:
- âœ… **TÃ­tulo**
- âœ… **AÃ±o de estreno**
- âœ… **CalificaciÃ³n** (IMDb rating)
- âœ… **DuraciÃ³n en minutos** (desde pÃ¡gina de detalle)
- âœ… **Metascore** (si estÃ¡ disponible)
- âœ… **Al menos 3 actores principales**

## ğŸ“ Estructura del Proyecto
```
imdb_movie_scraper/
â”œâ”€â”€ imdb_scraper
â”œâ”€â”€ sql_analysis
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ db
â”‚   â”œâ”€â”€ models
â”‚   â”œâ”€â”€ queries
â”‚   â”œâ”€â”€ routers
â”‚   â”œâ”€â”€ scripts
â”‚   â””â”€â”€ utils
â””â”€â”€ start.sh
â”œâ”€â”€ data
â”œâ”€â”€ docker-compose.db.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ runtime.txt
â”œâ”€â”€ README.md
```

## ğŸš€ InstalaciÃ³n y Uso

1. **Clona el repositorio:**

```bash
git clone https://github.com/migherize/imdb-movie-scraper.git
cd imdb-movie-scraper
```

2. **Copia el archivo `.env`:**

```bash
cp .env.example .env
```

### â–¶ï¸ OpciÃ³n 1: Sin Docker

1. **Crea y activa un entorno virtual:**

```bash
python -m venv venv
source venv/bin/activate  # Linux/macOS
venv\Scripts\activate     # Windows
```

2. **Instala dependencias:**

```bash
pip install -r requirements.txt
```

---

### ğŸ›¢ï¸ **Base de datos PostgreSQL (modo opcional)**

Si deseas usar PostgreSQL localmente (sin Docker), puedes crear la base de datos y sus tablas ejecutando el script `sql_analysis/utils/schema.sql` incluido en el proyecto.

#### âœ… Pasos:

1. **AsegÃºrate de tener PostgreSQL instalado** y que el servicio estÃ© en ejecuciÃ³n.

2. **Crea la base de datos manualmente** (si aÃºn no existe):

   ```bash
   createdb imdb
   ```

3. **Ejecuta el script SQL para crear las tablas:**

   ```bash
   psql -h localhost -U tu_usuario -d imdb -f sql_analysis/utils/schema.sql
   ```

   > Reemplaza `tu_usuario` por tu nombre de usuario en PostgreSQL.

#### ğŸ“„ Â¿QuÃ© incluye `schema.sql`?

Este archivo define la estructura base de la base de datos:

* `movies`: tabla con informaciÃ³n de pelÃ­culas (tÃ­tulo, aÃ±o, duraciÃ³n, rating, metascore).
* `actors`: tabla que almacena los actores, asociados a las pelÃ­culas por la clave forÃ¡nea `movie_id`.
* Ãndices y restricciones para mejorar la consulta.
* CreaciÃ³n de la vista `movie_actor_view` que relaciona pelÃ­culas con sus actores principales.

---

### ğŸ³ OpciÃ³n 2: Uso con Docker

Puedes levantar la base de datos y el servicio completo usando Docker.

#### 1. Levantar la base de datos PostgreSQL:

```bash
docker-compose -f docker-compose.db.yml up --build
```

#### 2. Levantar el servicio IMDB-MOVIE-SCRAPER:

```bash
docker-compose -f docker-compose.yml up --build
```

---

### âœ… Ejecutar los tests

1. AsegÃºrate de instalar las dependencias:

```bash
pip install -r requirements.txt
```

2. Ejecuta las pruebas con `pytest`:

```bash
pytest
```

ğŸ“¸ Resultado esperado:

![test](docs/test.png)

> **Nota:** El test `test_database_has_data` puede fallar si la base de datos estÃ¡ vacÃ­a. Esto es normal si aÃºn no has hecho el scraping de datos. El siguiente paso es ejecutar el scraper para poblar la base.

---
Claro, aquÃ­ tienes la secciÃ³n completada con una redacciÃ³n clara, profesional y consistente con el estilo del resto del README:

---

## ğŸ¬ Web Scraper: IMDB Movies

Para poblar la base de datos con informaciÃ³n de pelÃ­culas y actores:

```bash
cd imdb_scraper
scrapy crawl imdb_movies_spider
```

Si todo funciona correctamente, deberÃ­as ver el siguiente mensaje.

ğŸ“¸ Resultado esperado:

![FIN WEBSCRAPY](docs/WEBSCRAPY.png)

---

## ğŸ“Š Consultas SQL Avanzadas

Este proyecto incluye un conjunto de scripts para ejecutar anÃ¡lisis avanzados sobre la base de datos poblada desde IMDB.

Ejecuta los siguientes comandos desde la carpeta `scripts`:

```bash
cd scripts
python sql_analysis/scripts/run_query.py -a get_top_movies_by_decade
python sql_analysis/scripts/run_query.py -a get_standard_deviation_rating
python sql_analysis/scripts/run_query.py -a get_metascore_and_imdb_rating_normalizado
python sql_analysis/scripts/run_query.py -a create_view_actor_movie
python sql_analysis/scripts/run_query.py -a get_view_actor_movie
```

> **Nota:** cada query guarda automÃ¡ticamente su resultado en la carpeta `data/` con el nombre: `<nombre_de_la_query>.csv`

---

### 1. ğŸ¥ Top 5 pelÃ­culas con mayor duraciÃ³n por dÃ©cada

```bash
python run_query.py -a get_top_movies_by_decade
```

**DescripciÃ³n:**
Agrupa las pelÃ­culas por dÃ©cada de estreno y selecciona las 5 pelÃ­culas mÃ¡s largas (en duraciÃ³n) de cada grupo.

**Objetivo:**
Identificar tendencias temporales en la duraciÃ³n de las pelÃ­culas y destacar los filmes mÃ¡s extensos por Ã©poca.

---

### 2. ğŸ“ˆ DesviaciÃ³n estÃ¡ndar de calificaciones por aÃ±o

```bash
python run_query.py -a get_standard_deviation_rating
```

**DescripciÃ³n:**
Calcula la **desviaciÃ³n estÃ¡ndar** de las calificaciones IMDb para cada aÃ±o.

**Objetivo:**
Medir la dispersiÃ³n de opiniones de los usuarios en torno a las pelÃ­culas estrenadas cada aÃ±o. AÃ±os con mayor desviaciÃ³n indican variedad de calidad o polarizaciÃ³n.

---

### 3. âš ï¸ Diferencias entre calificaciones IMDb y Metascore

```bash
python run_query.py -a get_metascore_and_imdb_rating_normalizado
```

**DescripciÃ³n:**
Compara las calificaciones IMDb (escala 1â€“10) y Metascore (normalizada a 1â€“10) y detecta aquellas pelÃ­culas cuya diferencia supera el **20%**.

**Objetivo:**
Detectar discrepancias significativas entre la percepciÃ³n del pÃºblico general y la crÃ­tica profesional.

---

### 4. ğŸ§‘â€ğŸ¤â€ğŸ§‘ Crear vista: PelÃ­culas y actores principales

```bash
python run_query.py -a create_view_actor_movie
```

**DescripciÃ³n:**
Crea una **vista SQL** que relaciona cada pelÃ­cula con sus actores principales.

**Objetivo:**
Facilitar consultas rÃ¡pidas de pelÃ­culas en las que participa un actor especÃ­fico, sin necesidad de mÃºltiples `JOIN`.

> ğŸ’¡ Este paso **debe ejecutarse solo una vez** para crear la vista.

---

### 5. ğŸ” Consultar vista: Filtrar por actor

```bash
python run_query.py -a get_view_actor_movie
```

**DescripciÃ³n:**
Consulta la vista creada anteriormente, permitiendo filtrar pelÃ­culas por nombre de actor.

**Objetivo:**
Explorar fÃ¡cilmente la filmografÃ­a de un actor y su rol en distintas pelÃ­culas.

---

## ğŸ§  ComparaciÃ³n TÃ©cnica: Scrapy vs Selenium vs Playwright

### Â¿CÃ³mo implementarÃ­as este scraper usando **Playwright** o **Selenium**?

AquÃ­ algunos puntos clave a considerar:

* ğŸ”§ **ConfiguraciÃ³n avanzada del navegador**
  Puedes correr en modo `headless`, configurar `User-Agent`, `headers`, y tÃ©cnicas para evadir la detecciÃ³n de automatizaciÃ³n (`stealth`, `navigator.webdriver = false`, etc.).

* â³ **Selectores dinÃ¡micos y espera explÃ­cita**
  A diferencia de Scrapy, con Selenium y Playwright puedes esperar a que los elementos se rendericen completamente con `wait_for_selector` o `WebDriverWait`.

* ğŸ§© **Soporte completo para JavaScript y CAPTCHA**
  Si el sitio requiere ejecuciÃ³n de JS, login o tiene detecciÃ³n de bots, Selenium y Playwright permiten interactuar de manera visual (relleno de formularios, resoluciÃ³n de captchas con herramientas como 2Captcha o Playwright CAPTCHA plugin).

* âš™ï¸ **Control de concurrencia**
  Puedes usar herramientas como `ThreadPoolExecutor`, `async` (en Playwright), o colas distribuidas como `Celery` para scraping masivo.

### ğŸŸ¢ Â¿Por quÃ© usamos Scrapy?

* Scrapy es mÃ¡s **ligero y rÃ¡pido** para sitios estÃ¡ticos como IMDB.
* Tiene soporte nativo para middlewares, manejo de errores, y pipelines de datos.
* Mejor integraciÃ³n con proyectos de anÃ¡lisis de datos y control de volumen con `AutoThrottle`.

---

## ğŸš€ Usar la API con FastAPI (opcional)

Una vez poblada la base de datos, puedes consultar los datos fÃ¡cilmente desde la API REST:

![API](docs/API.png)

### â–¶ï¸ OpciÃ³n 1: Sin Docker

1. **Ejecuta la aplicaciÃ³n:**
```
uvicorn sql_analysis.main:app --reload
```

### ğŸ³ OpciÃ³n 2: Uso con Docker

1. **Accede a la documentaciÃ³n interactiva de FastAPI:**

```
http://localhost:8080/docs
```

Desde allÃ­ puedes ejecutar cada query directamente desde el navegador y obtener resultados en JSON.

ğŸ“¸ Ejemplo:

![example](docs/example.png)
